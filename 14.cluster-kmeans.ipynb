{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k均值聚类\n",
    "\n",
    "    14.3.2 策略\n",
    "    14.3.3 算法\n",
    "    14.3.4 算法特点\n",
    "\n",
    "k均值聚类是基于样本集合划分的聚类算法。\n",
    "k均值聚类将样本集合划分为k个子集，构成$k$个类，将$n$个样本分到$k$个类中，每个样本到其所属类的中心的距离最小。\n",
    "每个样本只能属于一个类，所以$k$均值聚类是硬聚类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.模型\n",
    "\n",
    "给定 $n$ 个样本的集合 $X = \\{x_1, x_2, \\cdots, x_n \\}$，每个样本由一个特征向量表示，特征向量的维数是 $m$ 。\n",
    "k均值聚类的目标是将 $n$ 个样本分到 $k$ 个不同的类或簇中，这里假设 $k<n$ 。\n",
    "$k$ 个类 $G_1, G_2, \\cdots, G_k $形成对样本集合 $X$ 的划分。\n",
    "一个划分对应着一个聚类结果。\n",
    "\n",
    "划分 $C$ 是一个多对一的函数。\n",
    "事实上，如果把每个样本用一个整数 $i \\in \\{1,2,\\cdots, n \\}$ 表示，每个类也用一个整数 $l \\in \\{1,2,\\cdots, k \\}$ 表示，那么划分或者聚\n",
    "类可以用函数 $l=C(i)$ 表示，其中 $i \\in \\{1,2,\\cdots, n \\}, l \\in \\{1,2,\\cdots, k \\}$。\n",
    "所以k均值聚类的模型是一个从样本到类的函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 策略\n",
    "\n",
    "k均值聚类归结为样本集合$X$的划分，或者从样本到类的函数的选择问题。\n",
    "k均值聚类的策略是通过损失函数的最小化选取最优的划分或函数$C^*$。\n",
    "\n",
    "首先，采用欧氏距离平方（squared Euclidean distance）作为样本之间的距离$d(x_i,x_j)$\n",
    "\n",
    "$$d_{(x_i,x_j)}=\\sum_{k=1}^{m}\\left(x_{k i}-x_{k j}\\right)^{2}$$\n",
    "\n",
    "然后，定义样本与其所属类的中心之间的距离的总和为损失函数，即\n",
    "\n",
    "$$W(C)=\\sum_{l=1}^{k} \\sum_{C(i)=l}\\|x_{i}-\\bar{x}_{l}\\|^{2}$$\n",
    "\n",
    "式中 $\\hat{x}_l = (\\hat{x}_{1l},\\hat{x}_{2l},\\cdots,\\hat{x}_{ml})^T$ 是第 $l$ 个类的均值或中心，$n_l=\\sum_{i=l}^n I(C(i)=l)$，$I(C(i)=l)$是指示函数，取值为 $1$ 或 $0$。\n",
    "函数 $W(C)$ 也称为能量，表系相同类中的样本相似的程度。\n",
    "\n",
    "相似的样本被聚到同类时，损失函数值最小，这个目标函数的最优化能达到聚类的目的。\n",
    "但是，这是一个组合优化问题，$n$个样本分到$k$类，所有可能分法的数目是指数级的。\n",
    "事实上，k均值聚类的最优解求解问题是NP hard问题。\n",
    "现实中采用迭代的方法求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.算法\n",
    "\n",
    "k均值聚类的算法是一个迭代的过程，每次迭代包括两个步骤。\n",
    "首先选择k个类的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果；\n",
    "然后更新每个类的样本的均值，作为类的新的中心；\n",
    "重复以上步骤，直到收敛为止。\n",
    "具体过程如下。\n",
    "\n",
    "首先，对于给定的中心值$m_1, m_2, \\cdots, m_k $，求一个划分$C$，使得目标函数极小化:\n",
    "\n",
    "$$\\min_C \\sum_{l=1}^{k} \\sum_{C(i)=l}\\left\\|x_{i}-m_{l}\\right\\|^{2}$$\n",
    "\n",
    "就是说在类中心确定的情况下，将每个样本分到一个类中，使样本和其所属类的中心之间的距离总和最小。\n",
    "求解结果，将每个样本指派到与其最近的中心$m_l$的类$G_l$中。\n",
    "\n",
    "然后，对给定的划分 $C$，再求各个类的中心 $m_1, m_2, \\cdots, m_k $， 使得目标函数极小化：\n",
    "\n",
    "$$\\min_{m_1, \\cdots, m_k} \\sum_{l=1}^{k} \\sum_{C(i)=l}\\left\\|x_{i}-m_{l}\\right\\|^{2}$$\n",
    "\n",
    "就是说在划分确定的情况下，使样本和其所属类的中心之间的距离总和最小。\n",
    "求解结果，对于每个包含 $n_l$ 个样本的类 $G_l$ 更新其均值 $m_l$：\n",
    "\n",
    "$$m_{l}=\\frac{1}{n_{l}} \\sum_{C(i)=l} x_{i}, \\quad l=1, \\cdots, k$$\n",
    "\n",
    "重复以上两个步骤，直到划分不再改变，得到聚类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k均值聚类算法\n",
    "\n",
    "输入：n个样本的集合X;\n",
    "输出：样本集合的聚类C\n",
    "\n",
    "1. 初始化。令 $t = 0$，随机选择 $k$ 个样本点作为初始聚类中心 $m^{(0)}=(m^{(0)}_1, \\cdots, m^{(0)}_l, \\cdots, m^{(0)}_k )$\n",
    "2. 对样本进行聚类。对固定的类中心 $m^{(t)}=(m^{(t)}_1, \\cdots, m^{(t)}_l, \\cdots, m^{(t)}_k )$，其中$m^{(t)}_l$为类$G_l$的中心，计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类中，构成聚类结果$C^{(t)}$。\n",
    "3. 计算新的类中心。对聚类结果$C^{(t)}$，计算当前各个类中的样本的均值，作为新的类中心$m^{(t+1)}=(m^{(t+1)}_1, \\cdots, m^{(t+1)}_l, \\cdots, m^{(t+1)}_k )$。\n",
    "4. 如果迭代收敛或符合停止条件，输出 $C^*=C^{(t)}$。否则，令 $t=t+1$，返回步（2)。\n",
    "\n",
    "k均值聚类算法的复杂度是$O(mnk)$，其中 $m$ 是样本维数，$n$ 是样本个数，$k$是类别个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法特性\n",
    "\n",
    "### 1. 总体特点\n",
    "\n",
    "k均值聚类有以下特点：基于划分的聚类方法；\n",
    "类别数k事先指定；以欧氏距离平方表示样本之间的距离，以中心或样本的均值表示类别；\n",
    "以样本和其所属类的中心之间的距离的总和为最优化的目标函数；\n",
    "得到的类别是平坦的、非层次化的；\n",
    "算法是迭代算法，不能保证得到全局最优。\n",
    "\n",
    "### 2. 收敛性\n",
    "\n",
    "k均值聚类属于启发式方法，不能保证收敛到全局最优，初始中心的选择会直接影响聚类结果。\n",
    "注意，类中心在聚类的过程中会发生移动，但是往往不会移动太大，因为在每一步，样本被分到与其最近的中心的类中。\n",
    "\n",
    "### 3. 初始类的选择\n",
    "\n",
    "选择不同的初始中心，会得到不同的聚类结果。\n",
    "初始中心的选择，比如可以用层次聚类对样本进行聚类，得到 k 个类时停止。\n",
    "然后从每个类中选取一个与中心距离最近的点。\n",
    "\n",
    "### 4. 类别数 k 的选择\n",
    "\n",
    "均值聚类中的类别数值需要预先指定，而在实际应用中最优的 k 值是不知道的。\n",
    "解决这个问题的一个方法是尝试用不同的 k 值聚类，检验各自得到聚类结果的质量，推测最优的 k 值 。\n",
    "聚类结果的质量可以用类的平均直径来衡量。\n",
    "一般地，类别数变小时，平均直径会增加；\n",
    "类别数变大超过某个值以后，平均直径会不变；\n",
    "而这个值正是最优的 k 值。\n",
    "实验时，可以采用二分査找，快速找到最优的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clustering.kmeans\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_means_plus_plus_center_indices(n, n_cluster, x, generator=np.random):\n",
    "    \"\"\"\n",
    "    :param n: number of samples in the data\n",
    "    :param n_cluster: the number of cluster centers required\n",
    "    :param x: data-  numpy array of points\n",
    "    :param generator: random number generator from 0 to n for choosing the first cluster at random\n",
    "            The default is np.random here but in grading, to calculate deterministic results,\n",
    "            We will be using our own random number generator.\n",
    "    :return: the center points array of length n_clusters with each entry being index to a sample\n",
    "             which is chosen as centroid.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select 1st center (randomly)\n",
    "    centers = [generator.randint(0, n)]\n",
    "\n",
    "    # Select other centers (with Euclidean distance)\n",
    "    for i in range(1, n_cluster):\n",
    "\n",
    "        \"\"\" The distance from each data point to the nearest cluster center \"\"\"\n",
    "        centers_point_list = x[np.array(centers)]\n",
    "        distance_matrix = spatial.distance.cdist(x, centers_point_list, metric=\"euclidean\")\n",
    "\n",
    "        \"\"\" Calculate the probability that each data point is selected as the next cluster center \"\"\"\n",
    "        prob_list = np.min(distance_matrix, axis=1)\n",
    "        prob_list = prob_list / np.sum(prob_list)\n",
    "\n",
    "        \"\"\" Select the data point with the highest prob \"\"\"\n",
    "        centers.append(np.argmax(prob_list))\n",
    "\n",
    "    print(\"[+] returning center for [{}, {}] points: {}\".format(n, len(x), centers))\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lloyd_k_means(n, n_cluster, _, generator):\n",
    "    return generator.choice(n, size=n_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\" Class KMeans:\n",
    "        Attr:\n",
    "            n_cluster - Number of cluster for k-means clustering (Int)\n",
    "            max_iter - maximum updates for k-means clustering (Int)\n",
    "            e - error tolerance (Float)\n",
    "            generator - random number generator from 0 to n for choosing the first cluster at random\n",
    "            The default is np.random here but in grading, to calculate deterministic results,\n",
    "            We will be using our own random number generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_cluster, max_iter=100, e=0.0001, generator=np.random):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.max_iter = max_iter\n",
    "        self.e = e\n",
    "        self.generator = generator\n",
    "\n",
    "        self.centers = None\n",
    "\n",
    "    def fit(self, x, centroid_func=get_lloyd_k_means):\n",
    "        \"\"\" Finds n_cluster in the data x\n",
    "            params:\n",
    "                x - N X D numpy array\n",
    "                centroid_func - To specify which algorithm we are using to compute the centers\n",
    "                eg: Lloyd(regular) or K-means++)\n",
    "            returns:\n",
    "                A tuple\n",
    "                    centroids: a n_cluster X D numpy array,\n",
    "                    y: a length (N,) numpy array where cell i is the ith sample's assigned cluster,\n",
    "                    number_of_updates: a Int)\n",
    "            Note: Number of iterations is the number of time you update the assignment\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"fit function takes 2-D numpy arrays as input\"\n",
    "        \n",
    "        n, d = x.shape\n",
    "\n",
    "        self.centers = centroid_func(n, self.n_cluster, x, self.generator)\n",
    "        centroids = x[self.centers]\n",
    "        y = np.zeros(n, dtype=np.int)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        \"\"\" Update means and membership until convergence or until you have made self.max_iter updates. \"\"\"\n",
    "        num_update = 0\n",
    "        for iter_id in tqdm(range(self.max_iter)):\n",
    "\n",
    "            # Compute distance\n",
    "            distance_matrix = spatial.distance.cdist(x, centroids)\n",
    "\n",
    "            # Membership for each data point\n",
    "            for i in range(n):\n",
    "                y[i] = np.argmin(distance_matrix[i])\n",
    "\n",
    "            # Update center of each cluster\n",
    "            for k in range(self.n_cluster):\n",
    "                centroids[k] = np.mean(x[np.where(y == k)[0]], axis=0)\n",
    "\n",
    "            # Compute loss\n",
    "            new_loss = 0\n",
    "            for i in range(n):\n",
    "                new_loss += distance_matrix[i][y[i]]\n",
    "\n",
    "            num_update = iter_id + 1\n",
    "\n",
    "            # Convergence check\n",
    "            error = abs(total_loss-new_loss)\n",
    "            if error < self.e * n:\n",
    "                break\n",
    "            else:\n",
    "                total_loss = new_loss\n",
    "\n",
    "        \"\"\" return (means, membership, number_of_updates) \"\"\"\n",
    "        return centroids, y, num_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClassifier:\n",
    "\n",
    "    \"\"\"\n",
    "        Class KMeansClassifier:\n",
    "        Attr:\n",
    "            n_cluster - Number of cluster for k-means clustering (Int)\n",
    "            max_iter - maximum updates for k-means clustering (Int)\n",
    "            e - error tolerance (Float)\n",
    "            generator - random number generator from 0 to n for choosing the first cluster at random\n",
    "            The default is np.random here but in grading, to calculate deterministic results,\n",
    "            We will be using our own random number generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_cluster, max_iter=100, e=1e-6, generator=np.random):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.max_iter = max_iter\n",
    "        self.e = e\n",
    "        self.generator = generator\n",
    "\n",
    "        self.centroids = None\n",
    "        self.centroid_labels = None\n",
    "\n",
    "    def fit(self, x, y, centroid_func=get_lloyd_k_means):\n",
    "        \"\"\" Train the classifier\n",
    "            params:\n",
    "                x - N X D size  numpy array\n",
    "                y - (N,) size numpy array of labels\n",
    "                centroid_func - To specify which algorithm we are using to compute the centers\n",
    "                eg: Lloyd(regular) or K-means++\n",
    "            returns:\n",
    "                None\n",
    "            Stores following attributes:\n",
    "                self.centroids : centroids obtained by k-means clustering (n_cluster X D numpy array)\n",
    "                self.centroid_labels : labels of each centroid obtained by majority voting (N,) numpy array)\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 2, \"x should be a 2-D numpy array\"\n",
    "        assert len(y.shape) == 1, \"y should be a 1-D numpy array\"\n",
    "        assert y.shape[0] == x.shape[0], \"y and x should have same rows\"\n",
    "\n",
    "        self.generator.seed(42)\n",
    "        n, d = x.shape\n",
    "\n",
    "        k_means = KMeans(self.n_cluster, self.max_iter, self.e, self.generator)\n",
    "        centroids, cluster_y, _ = k_means.fit(x, centroid_func)\n",
    "\n",
    "        centroid_labels = np.zeros(self.n_cluster, dtype=np.int)\n",
    "        for k in range(self.n_cluster):\n",
    "            label_y = y[np.where(cluster_y == k)[0]]\n",
    "            if len(label_y) == 0:\n",
    "                centroid_labels[k] = 0\n",
    "            else:\n",
    "                centroid_labels[k] = np.argmax(np.bincount(label_y))\n",
    "\n",
    "        self.centroid_labels = centroid_labels\n",
    "        self.centroids = centroids\n",
    "\n",
    "        assert self.centroid_labels.shape == (\n",
    "            self.n_cluster,), 'centroid_labels should be a numpy array of shape ({},)'.format(self.n_cluster)\n",
    "\n",
    "        assert self.centroids.shape == (\n",
    "            self.n_cluster, d), 'centroid should be a numpy array of shape {} X {}'.format(self.n_cluster, d)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Predict function\n",
    "            params:\n",
    "                x - N X D size  numpy array\n",
    "            returns:\n",
    "                predicted labels - numpy array of size (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(x.shape) == 2, \"x should be a 2-D numpy array\"\n",
    "\n",
    "        self.generator.seed(42)\n",
    "        n, d = x.shape\n",
    "\n",
    "        distance_matrix = spatial.distance.cdist(x, self.centroids)\n",
    "        labels = np.zeros(n, dtype=np.int)\n",
    "        for i in range(n):\n",
    "            labels[i] = self.centroid_labels[np.argmin(distance_matrix[i, :])]\n",
    "        return labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image, code_vectors):\n",
    "    \"\"\" Quantize image using the code_vectors\n",
    "        Return new image from the image by replacing each RGB value in image with nearest code vectors\n",
    "        (nearest in euclidean distance sense)\n",
    "        returns:\n",
    "            numpy array of shape image.shape\n",
    "    \"\"\"\n",
    "\n",
    "    assert image.shape[2] == 3 and len(image.shape) == 3, \\\n",
    "        'Image should be a 3-D array with size (?,?,3)'\n",
    "\n",
    "    assert code_vectors.shape[1] == 3 and len(code_vectors.shape) == 2, \\\n",
    "        'code_vectors should be a 2-D array with size (?,3)'\n",
    "\n",
    "    n, m, _ = image.shape\n",
    "\n",
    "    new_im = image.reshape(n * m, 3)\n",
    "\n",
    "    # Compute distance\n",
    "    distance_matrix = spatial.distance.cdist(new_im, code_vectors)\n",
    "\n",
    "    # Membership for each data point\n",
    "    for i in range(len(new_im)):\n",
    "        new_im[i] = code_vectors[np.argmin(distance_matrix[i])]\n",
    "\n",
    "    new_im = new_im.reshape(n, m, 3)\n",
    "\n",
    "    return new_im"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
